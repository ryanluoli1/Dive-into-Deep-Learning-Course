{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f533ba2b",
   "metadata": {},
   "source": [
    "# 1. Introduction to Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc84f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfcdef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"mps\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f602918",
   "metadata": {},
   "source": [
    "When dealing with images, the **traditional neural networks** have the following problems:\n",
    "\n",
    "1. too many pixels, **too many weights**\n",
    "2. **shifted** images will be ahrd to recognize\n",
    "3. **correlation** between pixles cannot be learnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e7961",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91147ebd",
   "metadata": {},
   "source": [
    "The **`convolution`** between two functions $f, g: \\mathbb{R}^d \\to \\mathbb{R}$ is defined as:\n",
    "\n",
    "$$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}$$\n",
    "\n",
    "It measures the **overlap** between $f$ and $g$ when one function is **“flipped” and shifted** by $x$. \n",
    "\n",
    "When the functions are **discrete**, we have:\n",
    "\n",
    "$$(f * g)(i) = \\sum_a f(a) g(i-a)$$\n",
    "\n",
    "For **2-dimensional tensors**, we have a corresponding sum with indices $(a,b)$ for $f$ and $(a-i,b-j)$ for $g$:\n",
    "\n",
    "$$(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e76e9",
   "metadata": {},
   "source": [
    "## Cross-Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ceba0",
   "metadata": {},
   "source": [
    "The **convolutional layer** is actually performing **`cross-correlation`** calculations instead of convolutions.\n",
    "\n",
    "Normally, an image is represented by a **3-dimensional** tensor. For example, a $10×10×3$ tensor represents an image with **$10×10$ pixels** and **$3$ colors** (RGB).\n",
    "\n",
    "Let's now ignore the **color channel** and just look at **cross-correlation** between the 2-dimensional tensor and the **kernel**:\n",
    "\n",
    "![ ](http://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "The first output is obtained by carrying out the following calculation:\n",
    "\n",
    "$$0×0+1×1+3×2+4×3=19$$\n",
    "\n",
    "Similarly, the other outputs are obatained by **sliding** (from left to right, from top to bottom) the kernel:\n",
    "\n",
    "$$\n",
    "1\\times0+2\\times1+4\\times2+5\\times3=25\\\\\n",
    "3\\times0+4\\times1+6\\times2+7\\times3=37\\\\\n",
    "4\\times0+5\\times1+7\\times2+8\\times3=43\n",
    "$$\n",
    "\n",
    "We can implement this calculation with the following code:\n",
    "\n",
    "```python \n",
    "Y[i,j] = (X[i:i+K_h,j:j+K_w]*K).sum()\n",
    "```\n",
    "\n",
    "In the above example, the ouput tensor is often referred as the **feature map** and the shaded area in the input tensor is called the **receptive field** of the output $19$.\n",
    "\n",
    "As you can see, the original tensor is **shrinked**. For a $n_h \\times n_w$ input and a $k_h \\times k_w$ kernel, the size of the output is given as\n",
    "\n",
    "$$(n_h-k_h+1) \\times (n_w-k_w+1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae78fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "    K_h, K_w = K.shape\n",
    "    Y = torch.zeros((X.shape[0]-K_h+1, X.shape[1]-K_w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i+K_h,j:j+K_w]*K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "941bac2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45e941",
   "metadata": {},
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db90f16",
   "metadata": {},
   "source": [
    "A **`convolutional layer`** first **cross-corrlates** the input and the kernel and then **adds a scalar bias** to produce an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a409088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.rand(kernel_size))     #initialize wthe eights (kernel)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))                 #initialize the scalar bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46718d",
   "metadata": {},
   "source": [
    "## Learning a Convolutional Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8deeeaa",
   "metadata": {},
   "source": [
    "Let's use backpropagation to learn the optimal kernel for a convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4a05274",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "\n",
    "K = torch.tensor([[1.0, -1.0]])\n",
    "Y = corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "233fe050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 14.254\n",
      "epoch 4, loss 3.940\n",
      "epoch 6, loss 1.296\n",
      "epoch 8, loss 0.477\n",
      "epoch 10, loss 0.187\n"
     ]
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(1,2), bias=False)\n",
    "\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "\n",
    "lr = 3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i+1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ae6e0da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0293, -0.9416]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e42def",
   "metadata": {},
   "source": [
    "To perform **convolution**, we need to **flip** the two-dimensional kernel tensor both **horizontally and vertically**, and then **cross-correlate** it with the input tensor.\n",
    "\n",
    "However, since the **convolutional kernels** are learn from the input data, performing **convolution** or **cross-correlation** will not affect the output of the convolutional layer.\n",
    "\n",
    "That is, if we use **convolution** instead of **cross-correlation** in learning, we will end up with a **kernel** that is equivalent to the kernel learned with cross-correlation but just flipped horizontally and veritcally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b2014",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec374a",
   "metadata": {},
   "source": [
    "Convolution usually **shrinks** the input data from $n_h \\times n_w$ to $(n_h-k_h+1) \\times (n_w-k_w+1)$:\n",
    "\n",
    "![ ](http://d2l.ai/_images/correlation.svg)\n",
    "\n",
    "When the input data is convolved **multiple times**, we tend to **lose pixels on the perimeter** of the image. \n",
    "\n",
    "One straightforward solution to this problem is to **add extra pixels of filler around the boundary** of the input image, thus **increasing the effective size** of the image. Typically, we set the values of the extra pixels to **zero**.\n",
    "\n",
    "This is known as **`padding`**:\n",
    "\n",
    "![](http://d2l.ai/_images/conv-pad.svg)\n",
    "\n",
    "If we add a total of **$p_h$ rows** of padding (roughly half on top and half on bottom) and a total of **$p_w$ columns** of padding (roughly half on the left and half on the right), the output shape becomes:\n",
    "\n",
    "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1)$$\n",
    "\n",
    "Therefore, to **maintain the size** of the input, we need to set $p_h=k_h-1$ and $p_w=k_w-1$. \n",
    "\n",
    "We usually use **odd** number of **kernel size** for the following benefits:\n",
    "\n",
    "1. equal padding on both sides of the input\n",
    "2. the output $Y[i,j]$ is the cross-correlation between the input window centered on $X[i, j]$ and the kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4800c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "    batch_size, n_channel = 1, 1\n",
    "    X = X.reshape((batch_size,n_channel)+X.shape)\n",
    "    Y = conv2d(X)\n",
    "    return Y.reshape(Y.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8462619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8,8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d6fdd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(5,3), padding=(2,1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5154b",
   "metadata": {},
   "source": [
    "## Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c77bb7",
   "metadata": {},
   "source": [
    "We refer to the number of rows and columns **traversed per slide** by the convolution kernel as **`stride`**.\n",
    "\n",
    "We use **larger strides** for better **computational efficiency** or when we wish to **downsample**. That is, the convolution kernel moves **more than one element** at a time. This is particularly useful when the kernel is large.\n",
    "\n",
    "For example, we set the horizontal stride $s_w=2$ and the vertical stride $s_h=3$, we have the following:\n",
    "\n",
    "![](http://d2l.ai/_images/conv-stride.svg)\n",
    "\n",
    "In general, when the stride for the height is $s_h$ and the stride for the width is $s_w$, the output shape is given as:\n",
    "\n",
    "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "984641ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aefd50a",
   "metadata": {},
   "source": [
    "## Multiple Input Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c320e5a",
   "metadata": {},
   "source": [
    "When we have **multiple input channels**, we will first cross-correlates the 2D input tensors with their corresponding 2D kernels (same channel), then add the results to obtain the output tensor:\n",
    "\n",
    "![](http://d2l.ai/_images/conv-multi-in.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "621c605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    return sum(corr2d(x,k) for x,k in zip(X,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "126e35c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "                 [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358bf8c7",
   "metadata": {},
   "source": [
    "## Multiple Output Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb9c20",
   "metadata": {},
   "source": [
    "To obtain **multiple output channels**, we can create a kernel with shape $c_i\\times k_h\\times k_w$ for each output channel, then concatenate them to obtain the convolution kernel with shape $c_o\\times c_i\\times k_h\\times k_w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d439e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61b1be4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.stack((K, K+1, K+2), 0)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afde341e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3b744",
   "metadata": {},
   "source": [
    "## $1 \\times 1$ Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa464c5",
   "metadata": {},
   "source": [
    "A **$1 \\times 1$ convolutional layer** will not capture the cross-correlations between pixels, but instead it outputs the **weighted linear combinations** of the same pixels in **different channels**:\n",
    "\n",
    "![](http://d2l.ai/_images/conv-1x1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05efa83f",
   "metadata": {},
   "source": [
    "Thus, the $1 \\times 1$ convolutional layer requires **$c_o\\times c_i$ weights** (plus the bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18d628de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h*w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_o, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79aad484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "\n",
    "float(torch.abs(Y1 - Y2).sum()) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77512cd",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4798f8",
   "metadata": {},
   "source": [
    "The **`pooling layers`** is used for two purposes:\n",
    "\n",
    "1. to **mitigate the sensitivity** of convolutional layers to **location**\n",
    "2. to **spatially downsampling** representations\n",
    "\n",
    "Similar to convolutional layers, pooling operators consist of a **fixed-shape window** that slides over the **entire region** of the input with a specified **stride size**. \n",
    "\n",
    "However, the pooling layer contains **no parameters** but instead is deterministic, typically calculating either the **maximum** or the **average** value of the elements in the pooling window.\n",
    "\n",
    "For example, a **maximum pooling** layer does the following:\n",
    "\n",
    "![](http://d2l.ai/_images/pooling.svg)\n",
    "\n",
    "where we have:\n",
    "\n",
    "$$\n",
    "\\max(0, 1, 3, 4)=4\\\\\n",
    "\\max(1, 2, 4, 5)=5\\\\\n",
    "\\max(3, 4, 6, 7)=7\\\\\n",
    "\\max(4, 5, 7, 8)=8\\\\\n",
    "$$\n",
    "\n",
    "A $p_w \\times p_h$ pooling layer **shrinks** the input tensor with shape $n_h \\times n_w$ to $(n_h-p_h+1) \\times (n_w-p_w+1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "408dcbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0]-p_h + 1, X.shape[1]-p_w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i,j] = X[i:i+p_h, j:j+p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i,j] = X[i:i+p_h, j:j+p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c99bc380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4f7f6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d980a",
   "metadata": {},
   "source": [
    "We can also change the **padding** and **stride** of the pooling layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f963a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2d = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d475307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aeea01",
   "metadata": {},
   "source": [
    "When there are multiple input channels, the pooling layer outputs the same number of channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99d0a0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.cat((X,X+1), 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62273bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
