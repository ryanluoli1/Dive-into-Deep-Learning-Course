{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21f21e35",
   "metadata": {},
   "source": [
    "# 6. Multi-Branch Networks (GoogLeNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2d093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4609d",
   "metadata": {},
   "source": [
    "The key contribution in GoogLeNet was that it solved the problem of **selecting convolution kernels**.\n",
    "\n",
    "While other works tried to identify the **optimal convolution kernel** with shape ranging from $1 \\times 1$ to $11 \\times 11$, GoogLeNet simply **concatenated multi-branch convolutions**.\n",
    "\n",
    "In the following, we introduce a **slightly simplified version** of GoodLeNet: some tricks are no longer needed due to the improvedf learning algorihtms nowedays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87995bb",
   "metadata": {},
   "source": [
    "## Inception Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f4171",
   "metadata": {},
   "source": [
    "The basic convolutional block in GoogLeNet is called an **`inception block`**:\n",
    "\n",
    "![](http://d2l.ai/_images/inception.svg)\n",
    "\n",
    "As shown above, an inception block consists of **4 parallel branches**. The first 3 branches use convolutional layers with window size $1 \\times 1$, $3 \\times 3$ and $5 \\times 5$.\\, trying to **extract information** from different spatial sizes. \n",
    "\n",
    "The middle two branches include a $1 \\times 1$ convolutional layer to reduce the number of channels and **model complexity**. The fourth branch uses a **max-pooling layer** followed by a $1 \\times 1$ convolutional layer to change the **number of channels**.\n",
    "\n",
    "All branches uses appropriate **padding** to maintain the size of the input. Finally, the outputs of all branches are **concatenated** along the channel dimension.\n",
    "\n",
    "The commonly tuned **hyperparameters** of the inception block are the **number of output channels per layer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0547cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        \n",
    "        #path 1: 1×1 convolutional layer\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        \n",
    "        #path 2: 1×1 convolutional layer + 3×3 convolution layer\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        \n",
    "        #path 3: 1×1 convolutional layer + 5×35 convolution layer\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        \n",
    "        #path 4: 3×3 max-pooling layer + 1×1 convolutional layer\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1899d",
   "metadata": {},
   "source": [
    "## GoogLeNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9db30",
   "metadata": {},
   "source": [
    "As shown below, **GoogLeNet** consists of **9 inception blocks** with **max-pooling** layers in between (to reduce dimensionality), and ends up with a **global average pooling** layer to generate outputs.\n",
    "\n",
    "![](http://d2l.ai/_images/inception-full-90.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f2b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a999fa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 192, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 480, 6, 6])\n",
      "Sequential output shape:\t torch.Size([1, 832, 3, 3])\n",
      "Sequential output shape:\t torch.Size([1, 1024])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 96, 96))\n",
    "\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
