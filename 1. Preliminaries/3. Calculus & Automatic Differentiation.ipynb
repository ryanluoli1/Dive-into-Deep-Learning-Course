{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculus & Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an n-dimensional vector $\\mathbf{x}=[x_1,x_2,\\ldots,x_n]^\\top$, the gradient of a function $f(\\mathbf{x})$ in respect to $\\mathbf{x}$ is given as:\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top$$\n",
    "\n",
    "The following rules applies:\n",
    "\n",
    "- For all $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, we have$\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$\n",
    "- For all $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$, we have$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$\n",
    "- For all $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, we have$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$\n",
    "- $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a differentiable function $y$ with variables $u_1, u_2, \\ldots, u_m$, each $u_i$ has variables $x_1, x_2, \\ldots, x_n$.\n",
    "\n",
    "For any $i = 1, 2, \\ldots, n$, the chain rule gives:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial u_1} \\frac{\\partial u_1}{\\partial x_i} + \\frac{\\partial y}{\\partial u_2} \\frac{\\partial u_2}{\\partial x_i} + \\cdots + \\frac{\\partial y}{\\partial u_m} \\frac{\\partial u_m}{\\partial x_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A deep learning framework like Pytorch can perform automaic differentiation. That is, it constructs a **computation graph** that getting track of the outputs useful for **backpropagation**.\n",
    "\n",
    "For example, we want to find the derivative of a **`scalar function`** $y=2\\mathbf{x}^{\\top}\\mathbf{x}$ in repsect to the vector $\\mathbf{x}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0, requires_grad=True)\n",
    "#x.requires_grad_(True)\n",
    "\n",
    "y = 2 * torch.dot(x.T, x)\n",
    "\n",
    "#back propagation on y\n",
    "y.backward()\n",
    "\n",
    "#gradients in respect to x\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got $4\\mathbf{x}$ which is the derivative of $y$ in respect to $x$.\n",
    "\n",
    "To compute the derivative for **`another function`**, we first need to zero out the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()         #zero out the gradients\n",
    "\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a **`non-scalar`** (multiple samples) function $y$, we aim to compute the sum of the derivatives for each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "\n",
    "y = x * x\n",
    "y.sum().backward()     #equivalent to y.backward(torch.ones(len(x)))\n",
    "\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to **`detach`** some computations.\n",
    "\n",
    "Consider two functions $y=x*x$ and $z=y*x$, we now want to compute the gradient of $z$ in respect to $x$ while having $y$ as a constant.\n",
    "\n",
    "That is, we want to have $\\frac{dz}{dx}=\\frac{d}{dx}(y*x)=y$ ($y$ as a constant) instead of $\\frac{dz}{dx}=\\frac{d}{dx}(x*x*x)=3x^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  3., 12., 27.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "\n",
    "y = x * x\n",
    "z = y * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 4., 9.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "\n",
    "y = x * x\n",
    "u = y.detach()           #create a new variable u by detaching y\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the derivatives of $y$ in respect to $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "\n",
    "y.sum().backward()\n",
    "\n",
    "x.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
