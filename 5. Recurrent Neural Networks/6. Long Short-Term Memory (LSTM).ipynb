{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be67005",
   "metadata": {},
   "source": [
    "# 6. Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30742270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d127e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94577ae3",
   "metadata": {},
   "source": [
    "Recall the structure of a **gated recurrent unit**:\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t$$\n",
    "\n",
    "![](http://d2l.ai/_images/gru-3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf5a24",
   "metadata": {},
   "source": [
    "## Gated Memory Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bda7b5",
   "metadata": {},
   "source": [
    "There are **3 gates** in a gated memory cell:\n",
    "\n",
    "1. **input gate**: pass the input\n",
    "2. **output gate**: pass the output\n",
    "3. **forget gate**: reset the states\n",
    "\n",
    "![](http://d2l.ai/_images/lstm-0.svg)\n",
    "\n",
    "Given $h$ **hidden states** and a **mini-batch** (with batch size $n$ and number of input $d$), we have the following:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i)\\\\\n",
    "\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)\\\\\n",
    "\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$ is the **input gate**, $\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$ is the **forget gate** and $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$ is the **output gate**.\n",
    "\n",
    "The values of these gates lie in the **range of (0,1)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01d83d",
   "metadata": {},
   "source": [
    "## Candidate Memory Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e841128",
   "metadata": {},
   "source": [
    "The **candidate memory cell** $\\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h}$ is given as:\n",
    "\n",
    "$$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$$\n",
    "\n",
    "![](http://d2l.ai/_images/lstm-1.svg)\n",
    "\n",
    "The **activation** used is the $\\tanh$ function, therefore, the values lies within the **range of (-1,1)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cab13d",
   "metadata": {},
   "source": [
    "## Memory Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4bd3e",
   "metadata": {},
   "source": [
    "The **input gate** $\\mathbf{I}_t$ governs how much we take **new data** into account via $\\tilde{\\mathbf{C}}_t$.\n",
    "\n",
    "The **forget gate** $\\mathbf{F}_t$ addresses how much of the **old cell internal state** we retain via $\\mathbf{C}_{t-1}$.\n",
    "\n",
    "Thus, the **memory cell** at the current time step is given as:\n",
    "\n",
    "$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$$\n",
    "\n",
    "When we have $\\mathbf{F}_t=1$ and $\\mathbf{I}_t=0$, we will be passing the exact same information stored in $\\mathbf{C}_{t-1}$ to $\\mathbf{C}_{t}$, vice versa. \n",
    "\n",
    "Such design alleviates the **vanishing gradient** problem and helps capturem the **long-term dependency** in the sequence.\n",
    "\n",
    "![](http://d2l.ai/_images/lstm-2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3402d2f1",
   "metadata": {},
   "source": [
    "## Hidden State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1dbfec",
   "metadata": {},
   "source": [
    "Finally, the **output gate** is used to obtain the **hidden state** $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$ at the current time step: \n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t)$$\n",
    "\n",
    "The values of the hidden state lies within the **range (-1,1)** because of the $\\tanh$ activation function.\n",
    "\n",
    "With $\\mathbf{O}_t=1$, we can pass all the information in the **memory cell** to the **prediction** at time step $t$. On the other hand, if $\\mathbf{O}_t=0$, we do not update the hidden state $\\mathbf{H}_t$.\n",
    "\n",
    "![](http://d2l.ai/_images/lstm-3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156512df",
   "metadata": {},
   "source": [
    "## Implementing LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ed0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = vocab_size\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens)\n",
    "\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
