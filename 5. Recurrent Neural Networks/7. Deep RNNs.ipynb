{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e535c5",
   "metadata": {},
   "source": [
    "# 7. Deep RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5076af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03d55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89df71",
   "metadata": {},
   "source": [
    "## Multiple Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62377585",
   "metadata": {},
   "source": [
    "The standard method for building **deep RNN** is strikingly simple: we **stack the RNNs on top of each other**. \n",
    "\n",
    "Any **RNN cell** at each time step depends on both the **same layer’s value at the previous time step** and the **previous layer’s value at the same time step**:\n",
    "\n",
    "![](http://d2l.ai/_images/deep-rnn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72af0e",
   "metadata": {},
   "source": [
    "Consider a **mini-batch** $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$, **hidden states** at the $l^\\mathrm{th}$ hidden layer  $\\mathbf{H}_t^{(l)}  \\in \\mathbb{R}^{n \\times h}$, and the corresponding **output** $\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$.\n",
    "\n",
    "We have $\\mathbf{H}_t^{(0)} = \\mathbf{X}_t$ and let the **activation** for the $l^\\mathrm{th}$ hidden layer be $\\phi_l$, then:\n",
    "\n",
    "$$\\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)})$$\n",
    "\n",
    "where $\\mathbf{W}_{xh}^{(l)} \\in \\mathbb{R}^{h \\times h}$ and $\\mathbf{W}_{hh}^{(l)} \\in \\mathbb{R}^{h \\times h}$ are the **weights** and $\\mathbf{b}_h^{(l)} \\in \\mathbb{R}^{1 \\times h}$ is the bias.\n",
    "\n",
    "The **final output** $\\mathbf{O}_t$ of the network depends only on the hidden state $\\mathbf{H}_t^{(L)}$ of the $L^\\mathrm{th}$ layer:\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{hq} + \\mathbf{b}_q$$\n",
    "\n",
    "where $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$ is the **weight** and $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$ is the **bias** of the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62d448",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121abf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, num_layers = len(vocab), 256, 2\n",
    "num_inputs = vocab_size\n",
    "\n",
    "device = torch.device('mps')\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)\n",
    "\n",
    "model = d2l.RNNModel(lstm_layer, vocab_size)\n",
    "model = model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
