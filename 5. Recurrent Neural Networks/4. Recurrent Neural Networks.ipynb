{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b882e82",
   "metadata": {},
   "source": [
    "# 4. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a369b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "27507920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee2d2a",
   "metadata": {},
   "source": [
    "## Hidden States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff5200e",
   "metadata": {},
   "source": [
    "When using **n-grams** in language modelling, we are modelling:\n",
    "\n",
    "$$P(x_t \\mid x_{t-1}, \\ldots, x_{t-n+1})$$\n",
    "\n",
    "However, the **number of parameters** increaase exponentially with $n$, where we need to store **$|\\mathcal{V}|^n$ numbers** for a vocabulary set $\\mathcal{V}$.\n",
    "\n",
    "Instead, we can use a **`hidden state`** $h_{t-1}$ which stores the **sequence information** up to time step $t-1$:\n",
    "\n",
    "$$P(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1})$$\n",
    "\n",
    "Generally, the hidden state at any time step $h_t$ could be computed based on both the **current input $x_t$** and the **previous hidden state $h_{t-1}$**:\n",
    "\n",
    "$$h_t = f(x_{t}, h_{t-1})$$\n",
    "\n",
    "Given that the function $f$ is **sufficiently powerful**, the above is not an approximation while reducing both the **costs of computation and storage**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa6a6d",
   "metadata": {},
   "source": [
    "## Neural Networks without Hidden States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329570d3",
   "metadata": {},
   "source": [
    "Let's consider a MLP with a single hidden layer.\n",
    "\n",
    "With the **activation function $\\phi$** and **mini-batch $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$**, the **output of the hidden layer** is given as:\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h)$$\n",
    "\n",
    "where we have the **weigths $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$** and **bias $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$**. ($h$ is number of hidden units)\n",
    "\n",
    "Then, the **final output $\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$** of the model is given as:\n",
    "\n",
    "$$\\mathbf{O} = \\mathbf{H} \\mathbf{W}_{hq} + \\mathbf{b}_q$$\n",
    "\n",
    "where we have the **weigths $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$** and **bias $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b2fc8",
   "metadata": {},
   "source": [
    "## Neural Networks with Hidden States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2405218",
   "metadata": {},
   "source": [
    "Let's consider that at time $t$, we have a **mini-batch input $\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$**, where each row of $\\mathbf{X}_t$ is a sample from the sequence at time $t$.\n",
    "\n",
    "Now, by storing the **hidden layer output $\\mathbf{H}_{t-1}$** from the **previous time step** and introducing a **new weight parameter $\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$**, we can compute the hidden layer output of the **current time step** as:\n",
    "\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h)$$\n",
    "\n",
    "As suggested in the above relationship, the **hidden states** captured and retained the sequence's historical information up to the current time step.\n",
    "\n",
    "Such computation is defined for all the hidden states and therefore is **recurrent**. Hence, neural networks with hidden states are called **recurrent neural neworks** and the layers performing these computations in RNNs are called **recurrent layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91343393",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ca8d5f",
   "metadata": {},
   "source": [
    "Given the **recurrent computation** of a RNN:\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h)$$\n",
    "\n",
    "The **output** is then:\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q$$\n",
    "\n",
    "**Parameters** of a RNN include the **weights $\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$ and bias $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$** of the hidden layers, and the **weights $\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$ and bias $\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$** for the output layer.\n",
    "\n",
    "It is worth mentioning that even at **different time steps**, RNNs always use these model parameters. Therefore, the **parameterization cost** of an RNN does not grow as the number of time steps increases.\n",
    "\n",
    "The following diagram demonstrates the computational logic of **3 connected recurrent layers**:\n",
    "\n",
    "![](http://d2l.ai/_images/rnn.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa2b8a",
   "metadata": {},
   "source": [
    "## Recurrent Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bd62b",
   "metadata": {},
   "source": [
    "The recurrent computation involves $\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}$ which can be proved as equivalent to the matrix multiplication between concatination $\\mathbf{X}_t$ and $\\mathbf{H}_{t-1}$ and the concatenation of $\\mathbf{W}_{xh}$ and $\\mathbf{W}_{hh}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32dcdba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
    "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abe136f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0326,  1.2618,  4.0260, -1.4869],\n",
       "        [ 1.5798,  0.6797, -2.0364,  1.6896],\n",
       "        [-0.3222,  0.1267,  1.0435,  0.6953]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d473015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0326,  1.2618,  4.0260, -1.4869],\n",
       "        [ 1.5798,  0.6797, -2.0364,  1.6896],\n",
       "        [-0.3222,  0.1267,  1.0435,  0.6953]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b36b8",
   "metadata": {},
   "source": [
    "## RNN-based Character-Level Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19159273",
   "metadata": {},
   "source": [
    "In language models, the goal is to **predict the next token** based on the current and previous tokens. Therefore, we **shift** the input sequences by one unit to obtaint the **labels**. \n",
    "\n",
    "Let's consider applying a **character-level language model** on the sequence **\"machine\"** with **batch size** of 1:\n",
    "\n",
    "![](http://d2l.ai/_images/rnn-train.svg)\n",
    "\n",
    "During the **training** process, we run a **softmax** operation on the **output** from the output layer for each time step, and then use the **cross-entropy loss** to compute the error between the model output and the target. \n",
    "\n",
    "In practice, each token is represented by a **$d$-dimensional vector**, and we use a **batch size** $n>1$. Therefore, the input $\\mathbf X_t$ at time step $t$ will be a **$n\\times d$ matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786f7af",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33ab11",
   "metadata": {},
   "source": [
    "Now, let's talk about how to **evaluate** a language model.\n",
    "\n",
    "To evaluate the **predictive ability** of a language model in predicting the **next token** based on the current and previous token, we can use the **average cross-entropy** of all the $n$ tokens in a sequence:\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1)$$\n",
    "\n",
    "where the **distribution $P$** is given by the model and $x_t$ is the **observed token** at time $t$.\n",
    "\n",
    "In NLP, instead of cross-entropy, we usually use an evaluation metric called **perplexity**:\n",
    "\n",
    "$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right)$$\n",
    "\n",
    "Perplexity can be best understood as the **geometric mean** of the number of **real choices** that we have when deciding which token to pick next. \n",
    "\n",
    "Letâ€™s look at a number of cases:\n",
    "\n",
    "1. In the **best** case scenario, the model always perfectly estimates the probability of the target token as 1. In this case the perplexity of the model is **1**.\n",
    "2. In the **worst** case scenario, the model always predicts the probability of the target token as 0. In this situation, the perplexity is **positive infinity**.\n",
    "3. At the **baseline**, the model predicts a uniform distribution over all the available tokens of the vocabulary. In this case, the perplexity equals the **number of unique tokens** of the vocabulary. In fact, this provides a nontrivial **upper bound** that any useful model must beat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c250c662",
   "metadata": {},
   "source": [
    "## Implementing RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c306d6",
   "metadata": {},
   "source": [
    "We first load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "79b28b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt', '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine(): \n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "def tokenize(lines, token='word'):\n",
    "    if token=='word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token=='char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('Error: wrong type of tokens.')\n",
    "        \n",
    "def count_corpus(tokens): \n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "class Vocab:  \n",
    "    \n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        \n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "            \n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self): \n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def load_corpus_time_machine(max_tokens=-1):  \n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'word')\n",
    "    vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus)-offset-1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset+num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset+1: offset+1+num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i:i+num_steps]\n",
    "        Y = Ys[:, i:i+num_steps]\n",
    "        yield X, Y\n",
    "\n",
    "class SeqDataLoader:  \n",
    "    \n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "    \n",
    "def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):\n",
    "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b94b23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a49fd9",
   "metadata": {},
   "source": [
    "Now, we define the **model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a3716fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48ccd79",
   "metadata": {},
   "source": [
    "Then, we initialize the **hidden states**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff6ebac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 256])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82e11f",
   "metadata": {},
   "source": [
    "Note that the **rnn_layer** does not computes the output but only returns the hidden states.\n",
    "\n",
    "(**Y** represents the all the **hidden states** at the 35 time steps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1018a423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 32, 4580])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "72d20759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb65a6",
   "metadata": {},
   "source": [
    "We need to define a class for the **complete RNN model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7a54b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        \n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens*2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            return (torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc1f3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5035da5",
   "metadata": {},
   "source": [
    "Let's have a look what the model would give us before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f1715301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix, num_preds, net, vocab, device): \n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]: \n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cf702d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tim<unk><unk>t<unk>av<unk><unk><unk><unk><unk>aaiandaaandiaa'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfae56b",
   "metadata": {},
   "source": [
    "Now, we **train** the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "856b1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta): \n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cea60c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)  \n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                state.detach_()\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7e83b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict_f = lambda prefix: predict(prefix, 50, net, vocab, device)\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict_f('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'Perplexity {ppl:.1f}, {speed:.1f} tokens/sec {str(device)}')\n",
    "    print(predict_f('time traveller'))\n",
    "    print(predict_f('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e23933",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e05bed",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24335da1",
   "metadata": {},
   "source": [
    "Let's have a look at the **gradients** in RNNs:\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial w_h}=\\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t} \\frac{\\partial f(x_{j},h_{j-1},w_h)}{\\partial h_{j-1}} \\right) \\frac{\\partial f(x_{i},h_{i-1},w_h)}{\\partial w_h}$$\n",
    "\n",
    "When the time step **$t$ gets larger**, the above equation gets longer, which might lead to **gradient vanishing** or **gradient exploding**.\n",
    "\n",
    "There is a few strategies available:\n",
    "\n",
    "> **Full Computation**: slow training, might cause gradient exploding, bad generalization\n",
    "\n",
    "> **Truncating Time Steps**: truncate the sum after $\\tau$ steps, giving an estimate $\\frac{\\partial h_{t-\\tau}}{\\partial w_h}$ of the true gradient, simple and stable, focuses on short-term effects, practically feasible\n",
    "\n",
    "> **Randomized Truncation**: replace the gradient with a random variable $z_t$ that truncates the sequence randomly,\n",
    ">$$z_t= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h} +\\xi_t \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_h}$$\n",
    ">where $E[z_t] = \\frac{\\partial h_{t-\\tau}}{\\partial w_h}$, $E[\\xi_t] = 1$, and $P(\\xi_t = 0) = 1-\\pi_t$, $P(\\xi_t = \\pi_t^{-1}) = \\pi_t$. \n",
    ">The summation ends whenever $\\xi_t = 0$. This results in a weighted sum of sequences with different lengths.\n",
    "\n",
    "![](http://d2l.ai/_images/truncated-bptt.svg)\n",
    "\n",
    "The above diagram demonstrates the 3 strategies: randomized truncation, regular truncation, full computation.\n",
    "\n",
    "Even though **randomized truncation** looks good, **regular truncation** works better in practice for the following reasons:\n",
    "\n",
    "1. The effect of an observation after a number of backpropagation steps into the past is sufficient to **capture dependencies** in practice.\n",
    "2. The **increase in variance** counteracts the fact that the gradient is more accurate with more steps.\n",
    "3. We want models that have only a **short range of interactions**.\n",
    "4. Regularly truncated backpropagation through time has a **slight regularizing effect** that can be desirable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
