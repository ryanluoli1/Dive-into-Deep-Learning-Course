{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Ouputs Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's consider the data has **four features** and **three classes**. Then, we need to have three **affine functions**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\n",
    "o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\n",
    "o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The neural network representation of this is:\n",
    "\n",
    "![](http://d2l.ai/_images/softmaxreg.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a **fully connected layer** with $d$ inputs and $q$ outputs, the number of parameters require is $\\mathcal{O}(dq)$. Practically, we can reduce this down to $\\mathcal{O}(\\frac{dq}{n})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to have the output $\\hat{y}_j$ as the **probability** of a data point belonging to class $j$.Then, we can obtain the **final output** as $\\operatorname*{argmax}_j y_j$.\n",
    "\n",
    "Thus, we can apply a **`softmax`** function over the outputs to make sure the they are all **positive** and **sum up to 1**.\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{where}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$$\n",
    "\n",
    "The softmax function does not change the **rank** of the original outputs from linear regression:\n",
    "\n",
    "$$\\operatorname*{argmax}_j \\hat y_j = \\operatorname*{argmax}_j o_j$$\n",
    "\n",
    "Thus, even though softmax is a **non-linear** function, softmax regression is still a **linear** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing the softmax regression, we normally read a **mini-batch** of the dataset $\\mathbf{X}$ with **dimension** $d$ and **batch size** $n$. \n",
    "\n",
    "If the dataset has $q$ **distinct classes**, we have: \n",
    "\n",
    "$$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$$\n",
    "$$\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$$\n",
    "$$\\mathbf{b} \\in \\mathbb{R}^{1\\times q}$$\n",
    "\n",
    "Then, the softmax regression is given as:\n",
    "\n",
    "$$ \\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned}$$\n",
    "\n",
    "where $\\mathbf{O}$ and $\\hat{\\mathbf{Y}}$ are both $\\in n \\times q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the **optimal parameters**, we aims to maximize the **likelihood**:\n",
    "\n",
    "$$P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})$$\n",
    "\n",
    "This is equivalent to minimizing the following:\n",
    "\n",
    "$$-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n",
    "= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)})$$\n",
    "\n",
    "where the **`loss function`** is the **cross-entropy**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l(\\mathbf{y}, \\hat{\\mathbf{y}}) &= - \\sum_{j=1}^q y_j \\log \\hat{y}_j \\\\\n",
    "&=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n",
    "&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\\\\\n",
    "&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, the **derivative** of the loss function in respect to the **linear output** $o_j$ is:\n",
    "\n",
    "$$\n",
    "\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
