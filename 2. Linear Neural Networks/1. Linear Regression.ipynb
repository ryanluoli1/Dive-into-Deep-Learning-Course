{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **`linear model`** is defined as:\n",
    "\n",
    "$$\\hat{y} = w_1  x_1 + ... + w_d  x_d + b$$\n",
    "\n",
    "If we store all the **features** in a vector $\\mathbf{x} \\in \\mathbb{R}^d$ and all the **weights** in a vector $\\mathbf{w} \\in \\mathbb{R}^d$, then we have:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b$$\n",
    "\n",
    "The **vectorized** version is given as:\n",
    "\n",
    "$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b$$\n",
    "\n",
    "What a linear model does is performing **affine transformation** on the input features which is a combiantion of **linear transformation** (by the weights) and **tranlation** (by the bias term)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used **loss function** is the **`sqaured error`**:\n",
    "\n",
    "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$\n",
    "\n",
    "where $\\hat{y}^{(i)}$ is the **prediction** on the $i^{th}$ sample and $y^{(i)}$ is the **true label**.\n",
    "\n",
    "The **cost function** over the entire dataset is given as:\n",
    "\n",
    "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2$$\n",
    "\n",
    "The **optimal parameters** are found by minimizing the cost function:\n",
    "\n",
    "$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`analytical solution`** of a linear regression model is given as:\n",
    "\n",
    "$$\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}$$\n",
    "\n",
    "by minimizing the cost function $\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **`gradient descent`** to find the **optimal parameters**.\n",
    "\n",
    "To speed up training, we usually compute the **derivatives** on a radnomly sampled small subset (a **mini batch** $\\mathcal{B}$) of the training data.\n",
    "\n",
    "Then, we update the parameters according to a pre-set **learning-rate** $\\eta$:\n",
    "\n",
    "$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)$$\n",
    "\n",
    "More specifically, we have:\n",
    "\n",
    "$$\\begin{aligned} \\mathbf{w} &\\leftarrow \\mathbf{w} -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{\\mathbf{w}} l^{(i)}(\\mathbf{w}, b) = \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right),\\\\ b &\\leftarrow b -  \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_b l^{(i)}(\\mathbf{w}, b)  = b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right). \\end{aligned}$$\n",
    "\n",
    "where $|\\mathcal{B}|$ is the **batch size**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a **normally distributed** random variable $x$, if it has mean $\\mu$ and variance $\\sigma^2$, then the **probability density function** is given as\n",
    "\n",
    "$$p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (x - \\mu)^2\\right)$$\n",
    "\n",
    "The reason why **square error** can be used as a loss function for linear regression is based on the assumption that the **noise** $\\epsilon$ in the data is **normally distributed**:\n",
    "\n",
    "$$y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "Thus, the **likelihood** of $y$ given $x$ can be written as:\n",
    "\n",
    "$$P(y \\mid \\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right)$$\n",
    "\n",
    "According to the **`maximum likelihood estimation`**, we aim to have the parameters $w$ and $b$ that maximize the **likelihood** of the entire training set:\n",
    "\n",
    "$$P(\\mathbf y \\mid \\mathbf X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)})$$\n",
    "\n",
    "This is equivalent to minimizing the **log likelihood**:\n",
    "\n",
    "$$-\\log P(\\mathbf y \\mid \\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2$$\n",
    "\n",
    "Given that $\\sigma$ is a constant, the above minimization is equivalent to minimizing the sqaure error loss function. Therefore, under the assumption of normality, the **sqaure error is the same as maximum likelihood estimation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://d2l.ai/_images/singleneuron.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
