{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Softmax Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator: \n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    '''\n",
    "    Function to obtain the text label for each data point.\n",
    "    '''\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size, resize=None):\n",
    "    '''\n",
    "    Function to create a data iterator.\n",
    "    '''\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root='../data', train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root='../data', train=False, transform=trans, download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=4),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(),             #flatten the image matrices\n",
    "                    nn.Linear(28*28, 10)      #fully connected layer\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net):\n",
    "    if type(net) == nn.Linear:\n",
    "        nn.init.normal_(net.weight, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **softmax function** is given as:\n",
    "\n",
    "$$\\hat y_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$$\n",
    "\n",
    "To prevent potential **overflow** caused by the exponential term, we can **subtract $\\max(o_k)$ from $o_k$** before computing softmax:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat y_j & =  \\frac{\\exp(o_j - \\max(o_k))\\exp(\\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))\\exp(\\max(o_k))} \\\\\n",
    "& = \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which is equivalent to the original softmax function.\n",
    "\n",
    "Now, to prevent potential **underflow** caused by the term $\\log(\\hat y_j)$ where $\\hat y_j=\\exp(o_j - \\max(o_k))$, we can **avoid such calculation** by obtaining:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log{(\\hat y_j)} & = \\log\\left( \\frac{\\exp(o_j - \\max(o_k))}{\\sum_k \\exp(o_k - \\max(o_k))}\\right) \\\\\n",
    "& = \\log{(\\exp(o_j - \\max(o_k)))}-\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)} \\\\\n",
    "& = o_j - \\max(o_k) -\\log{\\left( \\sum_k \\exp(o_k - \\max(o_k)) \\right)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By doing this, we can keep the **softmax function** to output probabilities while using the **original linear outputs** when calculating the the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    '''\n",
    "    Count the number of correctly predicted samples.\n",
    "    '''\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1) \n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):\n",
    "    '''\n",
    "    Cumulative evaluation over multiple mini-batches.\n",
    "    '''\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  #set the model to evaluation mode\n",
    "    metric = Accumulator(2)  #number of correct predictions and total predictions\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater):\n",
    "    '''\n",
    "    Training for a single epoch.\n",
    "    '''\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()   \n",
    "    metric = Accumulator(3)      #training loss, training accuracy, sample size\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy(y_hat,y), y.numel())\n",
    "    return metric[0]/metric[2], metric[1]/metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(f'Training loss: {train_metrics[0]}')\n",
    "        print(f'Training accuracy: {train_metrics[1]}')\n",
    "        print(f'Test accuracy: {test_acc}')\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training loss: 0.7777179353713989\n",
      "Training accuracy: 0.7530833333333333\n",
      "Test accuracy: 0.7947\n",
      "Epoch 2\n",
      "Training loss: 0.5702055841445923\n",
      "Training accuracy: 0.813\n",
      "Test accuracy: 0.8085\n",
      "Epoch 3\n",
      "Training loss: 0.5256161816279094\n",
      "Training accuracy: 0.8250333333333333\n",
      "Test accuracy: 0.8145\n",
      "Epoch 4\n",
      "Training loss: 0.5008067871729532\n",
      "Training accuracy: 0.83285\n",
      "Test accuracy: 0.8271\n",
      "Epoch 5\n",
      "Training loss: 0.48495729923248293\n",
      "Training accuracy: 0.8365\n",
      "Test accuracy: 0.8253\n",
      "Epoch 6\n",
      "Training loss: 0.4740870672225952\n",
      "Training accuracy: 0.8408166666666667\n",
      "Test accuracy: 0.8221\n",
      "Epoch 7\n",
      "Training loss: 0.46608039538065593\n",
      "Training accuracy: 0.8425666666666667\n",
      "Test accuracy: 0.8224\n",
      "Epoch 8\n",
      "Training loss: 0.45856751906077065\n",
      "Training accuracy: 0.8443333333333334\n",
      "Test accuracy: 0.8303\n",
      "Epoch 9\n",
      "Training loss: 0.45250986830393475\n",
      "Training accuracy: 0.8466833333333333\n",
      "Test accuracy: 0.8226\n",
      "Epoch 10\n",
      "Training loss: 0.447976597849528\n",
      "Training accuracy: 0.8468833333333333\n",
      "Test accuracy: 0.8303\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
